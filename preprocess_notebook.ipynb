{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5471b48e-bad6-47f3-a6f3-0d3a4ba0c397",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/philipleong/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/philipleong/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/philipleong/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/philipleong/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/philipleong/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from textblob import TextBlob\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, ENGLISH_STOP_WORDS\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.decomposition import PCA\n",
    "import umap\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from io import BytesIO\n",
    "import boto3\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load SpaCy model for Named Entity Recognition\n",
    "NER = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "deab07fb-c1d9-47dc-9f9c-7eb10a447c99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded news_excerpts_parsed.xlsx: 1509 rows, 2 columns\n",
      "‚úÖ All datasets loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "s3_client = boto3.client(\n",
    "    's3',\n",
    "    aws_access_key_id='',\n",
    "    aws_secret_access_key=''\n",
    ")\n",
    "\n",
    "bucket_name = 'bia-data'\n",
    "news_file_key = 'news_excerpts_parsed.xlsx'\n",
    "\n",
    "def load_excel_from_s3(bucket, file_key):\n",
    "    \"\"\"\n",
    "    Downloads an Excel file from S3 and loads it into a Pandas DataFrame.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Fetch the file from S3\n",
    "        file_obj = s3_client.get_object(Bucket=bucket, Key=file_key)\n",
    "        \n",
    "        # Read the file into a Pandas DataFrame\n",
    "        df = pd.read_excel(BytesIO(file_obj['Body'].read()))\n",
    "        \n",
    "        print(f\"‚úÖ Loaded {file_key}: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to load {file_key} from S3: {e}\")\n",
    "        return None  # Return None if the file is missing or an error occurs\n",
    "\n",
    "# Load Datasets from S3\n",
    "news_excerpts = load_excel_from_s3(bucket_name, news_file_key)\n",
    "\n",
    "# Check if DataFrames are loaded successfully\n",
    "if news_excerpts is not None:\n",
    "    print(\"‚úÖ All datasets loaded successfully!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Error in loading datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "20ff7c83-a8ea-4fc8-8307-5262d832d9f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting analysis...\n",
      "Cleaning and preprocessing text...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1509/1509 [00:00<00:00, 3371.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting countries and calculating sentiment...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1509/1509 [00:00<00:00, 29605.19it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1509/1509 [00:00<00:00, 3947.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering articles...\n",
      "üöÄ Uploaded text_analysis_for_tableau.xlsx to s3://bia-data/exports/text_analysis_for_tableau.xlsx\n",
      "\n",
      "Extracting relationships...\n",
      "üöÄ Uploaded relationships_output.xlsx to s3://bia-data/exports/relationships_output.xlsx\n",
      "üöÄ Uploaded relationship_keywords_output.xlsx to s3://bia-data/exports/relationship_keywords_output.xlsx\n",
      "üöÄ Successfully uploaded combined_analysis.xlsx to s3://bia-data/exports/combined_analysis.xlsx\n",
      "‚úÖ Combined Excel file uploaded to: s3://bia-data/exports/combined_analysis.xlsx\n",
      "Analysis complete!\n"
     ]
    }
   ],
   "source": [
    "COUNTRIES = {\n",
    "    'United States': ['United States', 'US', 'USA', 'U.S.', 'America', 'Washington', 'New York', 'California'],\n",
    "    'China': ['China', 'Chinese', 'Beijing', 'Shanghai', 'Shenzhen', 'Hong Kong'],\n",
    "    'Singapore': ['Singapore', 'Singaporean'],\n",
    "    'United Kingdom': ['UK', 'Britain', 'England', 'London', 'United Kingdom', 'British'],\n",
    "    'European Union': ['EU', 'European Union', 'Brussels', 'Europe', 'European'],\n",
    "    'Japan': ['Japan', 'Japanese', 'Tokyo', 'Osaka'],\n",
    "    'India': ['India', 'Indian', 'New Delhi', 'Mumbai', 'Bangalore'],\n",
    "    'Australia': ['Australia', 'Australian', 'Sydney', 'Melbourne', 'Canberra'],\n",
    "    'Canada': ['Canada', 'Canadian', 'Toronto', 'Ottawa', 'Vancouver'],\n",
    "    'Germany': ['Germany', 'German', 'Berlin', 'Frankfurt'],\n",
    "    'France': ['France', 'French', 'Paris'],\n",
    "    'Russia': ['Russia', 'Russian', 'Moscow'],\n",
    "    'South Korea': ['South Korea', 'Korean', 'Seoul'],\n",
    "    'Brazil': ['Brazil', 'Brazilian', 'Sao Paulo', 'Rio'],\n",
    "    'Indonesia': ['Indonesia', 'Indonesian', 'Jakarta'],\n",
    "    'Thailand': ['Thailand', 'Thai', 'Bangkok'],\n",
    "    'Vietnam': ['Vietnam', 'Vietnamese', 'Hanoi'],\n",
    "    'Malaysia': ['Malaysia', 'Malaysian', 'Kuala Lumpur']\n",
    "}\n",
    "\n",
    "def add_id_column(df, column_name='Article Id', start_from=1):\n",
    "    # Create a copy to avoid modifying the original DataFrame\n",
    "    result = df.copy()\n",
    "    \n",
    "    # Add ID column\n",
    "    result.insert(0, column_name, range(start_from, len(df) + start_from))\n",
    "    \n",
    "    return result\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean and preprocess text with enhanced stop word removal.\"\"\"\n",
    "    # Create custom stop words set\n",
    "    custom_stop_words = set(stopwords.words('english')).union(ENGLISH_STOP_WORDS)\n",
    "    custom_stop_words.update(['said', 'says', 'tell', 'told'])\n",
    "    \n",
    "    # Lowercase and remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text.lower())\n",
    "    \n",
    "    # Remove stop words\n",
    "    words = text.split()\n",
    "    cleaned_words = [word for word in words if word not in custom_stop_words]\n",
    "    \n",
    "    return ' '.join(cleaned_words)\n",
    "\n",
    "stop_words = set(stopwords.words('english')).union(ENGLISH_STOP_WORDS)\n",
    "stop_words.update(['said', 'says', 'tell', 'told'])  # Custom stop words\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Clean and preprocess text.\"\"\"\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Extract country mentions\n",
    "def extract_countries_from_text(text):\n",
    "    \"\"\"Extract countries and their counts from text.\"\"\"\n",
    "    text = text.lower()\n",
    "    country_mentions = []\n",
    "    for country, keywords in COUNTRIES.items():\n",
    "        for keyword in keywords:\n",
    "            if keyword.lower() in text:\n",
    "                country_mentions.append(country)\n",
    "                break\n",
    "    if country_mentions:\n",
    "        country_counts = Counter(country_mentions)\n",
    "        most_mentioned = max(country_counts.items(), key=lambda x: x[1])[0]\n",
    "        return {'primary_country': most_mentioned, 'all_countries': dict(country_counts)}\n",
    "    return {'primary_country': None, 'all_countries': {}}\n",
    "\n",
    "# Sentiment analysis\n",
    "def get_sentiment(text):\n",
    "    \"\"\"Calculate sentiment polarity.\"\"\"\n",
    "    blob = TextBlob(str(text))\n",
    "    return round(blob.sentiment.polarity, 4)\n",
    "\n",
    "# Extract cluster keywords\n",
    "def get_cluster_terms_frequencies(vectorizer, kmeans, tfidf_matrix, n_terms=10):\n",
    "    \"\"\"Get top terms and their frequencies for each cluster.\"\"\"\n",
    "    terms = vectorizer.get_feature_names_out()\n",
    "    clusters = kmeans.labels_\n",
    "    cluster_term_freq = []\n",
    "    for cluster_idx in range(kmeans.n_clusters):\n",
    "        cluster_docs = tfidf_matrix[clusters == cluster_idx]\n",
    "        avg_tfidf = cluster_docs.mean(axis=0).A1\n",
    "        top_indices = avg_tfidf.argsort()[-n_terms:][::-1]\n",
    "        for idx in top_indices:\n",
    "            cluster_term_freq.append({'Cluster': cluster_idx, 'Term': terms[idx], 'Frequency': avg_tfidf[idx]})\n",
    "    return pd.DataFrame(cluster_term_freq)\n",
    "\n",
    "def process_articles(df):\n",
    "    \"\"\"Clean, analyze, and cluster articles.\"\"\"\n",
    "    print(\"Cleaning and preprocessing text...\")\n",
    "    tqdm.pandas()\n",
    "    df['cleaned_text'] = df['Text'].progress_apply(preprocess_text)\n",
    "\n",
    "    print(\"Extracting countries and calculating sentiment...\")\n",
    "    countries_data = df['Text'].progress_apply(extract_countries_from_text)\n",
    "    df['primary_country'] = countries_data.apply(lambda x: x['primary_country'])\n",
    "    df['all_countries'] = countries_data.apply(lambda x: x['all_countries'])\n",
    "    df['sentiment'] = df['Text'].progress_apply(get_sentiment)\n",
    "\n",
    "    print(\"Clustering articles...\")\n",
    "    vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')\n",
    "    tfidf_matrix = vectorizer.fit_transform(df['cleaned_text'])\n",
    "    kmeans = KMeans(n_clusters=5, random_state=42)\n",
    "    df['cluster'] = kmeans.fit_predict(tfidf_matrix)\n",
    "    term_freq_df = get_cluster_terms_frequencies(vectorizer, kmeans, tfidf_matrix)\n",
    "    return df, term_freq_df\n",
    "\n",
    "def extract_relationships(doc, nlp):\n",
    "    \"\"\"Extract relationships using rule-based and dependency-based methods.\"\"\"\n",
    "    relationships = []\n",
    "    \n",
    "    # Initialize spaCy Matcher\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    \n",
    "    # Rule 1: X and Y (compound relationships)\n",
    "    pattern_and = [{\"POS\": \"PROPN\"}, {\"LOWER\": \"and\"}, {\"POS\": \"PROPN\"}]\n",
    "    matcher.add(\"AND_PATTERN\", [pattern_and])\n",
    "    \n",
    "    # Rule 2: X's Y (possessive relationships)\n",
    "    pattern_possessive = [{\"POS\": \"PROPN\"}, {\"LOWER\": \"'s\"}, {\"POS\": \"NOUN\"}]\n",
    "    matcher.add(\"POSSESSIVE_PATTERN\", [pattern_possessive])\n",
    "    \n",
    "    # Rule 3: X is Y (appositive relationships)\n",
    "    pattern_appositive = [{\"POS\": \"PROPN\"}, {\"LOWER\": \"is\"}, {\"POS\": \"PROPN\"}]\n",
    "    matcher.add(\"APPOSITIVE_PATTERN\", [pattern_appositive])\n",
    "    \n",
    "    # Rule 4: X who is Y (relative clause relationships)\n",
    "    pattern_relative_clause = [{\"POS\": \"PROPN\"}, {\"LOWER\": \"who\"}, {\"LOWER\": \"is\"}, {\"POS\": \"NOUN\"}]\n",
    "    matcher.add(\"RELATIVE_CLAUSE_PATTERN\", [pattern_relative_clause])\n",
    "    \n",
    "    # Rule 5: X quickly Y (verb phrases with modifiers)\n",
    "    pattern_verb_phrase = [{\"POS\": \"PROPN\"}, {\"POS\": \"ADV\"}, {\"POS\": \"VERB\"}]\n",
    "    matcher.add(\"VERB_PHRASE_PATTERN\", [pattern_verb_phrase])\n",
    "    \n",
    "    # Find matches in the document\n",
    "    matches = matcher(doc)\n",
    "    for match_id, start, end in matches:\n",
    "        span = doc[start:end]\n",
    "        match_label = nlp.vocab.strings[match_id]\n",
    "        relationships.append((\"Rule-based\", span.text))\n",
    "    \n",
    "    # Dependency-based extraction\n",
    "    for token in doc:\n",
    "        if token.dep_ in (\"nsubj\", \"dobj\", \"pobj\"):  # Focus on subject, object relationships\n",
    "            if token.head.pos_ in (\"VERB\", \"NOUN\"):\n",
    "                if token.dep_ == \"nsubj\":\n",
    "                    rel_type = \"SUBJECT_OF\"\n",
    "                elif token.dep_ == \"dobj\":\n",
    "                    rel_type = \"OBJECT_OF\"\n",
    "                elif token.dep_ == \"pobj\":\n",
    "                    rel_type = \"OBJECT_OF_PREPOSITION\"\n",
    "                relationships.append((rel_type, f\"{token.text} -> {token.head.text}\"))\n",
    "    \n",
    "    return relationships\n",
    "\n",
    "def save_relationships_to_excel(relationships, file_name=\"relationships_output.xlsx\"):\n",
    "    \"\"\"Save relationships to an Excel file.\"\"\"\n",
    "    df = pd.DataFrame(relationships, columns=[\"Relationship Type\", \"Source Entity\", \"Target Entity\", \n",
    "                                              \"Cluster Number\", \"Distance to Centroid\"])\n",
    "    # Create an in-memory Excel file\n",
    "    output = BytesIO()\n",
    "    with pd.ExcelWriter(output, engine='openpyxl') as writer:\n",
    "        df.to_excel(writer, sheet_name=\"Extracted Relationships\", index=False)\n",
    "    output.seek(0)  # Reset file pointer\n",
    "\n",
    "    s3_folder = 'exports'\n",
    "    s3_file_key = f\"{s3_folder}/{file_name}\"\n",
    "\n",
    "    # Upload the Excel file to S3\n",
    "    s3_client.put_object(Bucket=bucket_name, Key=s3_file_key, Body=output.getvalue())\n",
    "    \n",
    "    print(f\"üöÄ Uploaded {file_name} to s3://{bucket_name}/{s3_file_key}\")\n",
    "    return df\n",
    "\n",
    "def k_means_clustering_and_extract_relationships(df, text_column, n_clusters=5):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    \"\"\"Perform KMeans clustering and extract relationships.\"\"\"\n",
    "    # Clean text\n",
    "    df[text_column] = df[text_column].apply(clean_text)\n",
    "    \n",
    "    # Extract text\n",
    "    documents = df[text_column].dropna().tolist()\n",
    "    \n",
    "    # TF-IDF Vectorization\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    vectorized_texts = vectorizer.fit_transform(documents)\n",
    "    \n",
    "    # Perform KMeans clustering\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    cluster_labels = kmeans.fit_predict(vectorized_texts)\n",
    "    \n",
    "    # Calculate distances to cluster centroids\n",
    "    distances = kmeans.transform(vectorized_texts)\n",
    "    \n",
    "    all_relationships = []  # To store all extracted relationships\n",
    "    \n",
    "    # Extract relationships from each cluster\n",
    "    for cluster in range(n_clusters):\n",
    "        cluster_docs = []\n",
    "        cluster_distances = []\n",
    "        \n",
    "        # Collect documents and their distances for this cluster\n",
    "        for idx, (doc, label) in enumerate(zip(documents, cluster_labels)):\n",
    "            if label == cluster:\n",
    "                cluster_docs.append(doc)\n",
    "                cluster_distances.append(distances[idx][cluster])\n",
    "        \n",
    "        cluster_text = ' '.join(cluster_docs)\n",
    "        \n",
    "        # Process text with spaCy\n",
    "        doc = nlp(cluster_text)\n",
    "        \n",
    "        # Extract relationships\n",
    "        relationships = extract_relationships(doc, nlp)\n",
    "        \n",
    "        # Organize relationships with source and target entities\n",
    "        for rel_type, rel_text in relationships:\n",
    "            if \"->\" in rel_text:  # Dependency-based relationships\n",
    "                source, target = rel_text.split(\" -> \")\n",
    "                # Use the minimum distance to centroid for this relationship\n",
    "                min_distance = min(cluster_distances)\n",
    "                all_relationships.append([rel_type, source, target, cluster, min_distance])\n",
    "            else:  # Rule-based relationships\n",
    "                entities = rel_text.split()\n",
    "                if len(entities) >= 2:\n",
    "                    source, target = entities[0], entities[-1]\n",
    "                    min_distance = min(cluster_distances)\n",
    "                    all_relationships.append([rel_type, source, target, cluster, min_distance])\n",
    "        \n",
    "    # Save relationships to Excel\n",
    "    df = save_relationships_to_excel(all_relationships)\n",
    "    return df\n",
    "    \n",
    "def filter_words(node_list, count_dict, num_keywords, threshold=3):\n",
    "    '''returns the top num_keywords occuring keywords, default 50'''\n",
    "        \n",
    "    # set dynamic threshold, limit number of keywords to 50\n",
    "    filtered = node_list.copy()\n",
    "    num_keywords = min(num_keywords,  0.1 * len(node_list))\n",
    "        \n",
    "    while (len(filtered) > num_keywords or len(filtered) == 0):\n",
    "        for node in filtered:\n",
    "            if count_dict.get(node, 0) < threshold:\n",
    "                    filtered.remove(node)\n",
    "        threshold += 1\n",
    "            \n",
    "    return filtered\n",
    "\n",
    "def get_relationship_key_words(df, num_clusters=5, num_keywords=50):\n",
    "    '''returns top num_keywords keywords per cluster'''\n",
    "    cluster_keywords = []\n",
    "    for i in range(num_clusters):\n",
    "        cluster_df = df[df['Cluster Number'] == i]\n",
    "        source_names = cluster_df['Source Entity'].to_list()\n",
    "        source_names = list(dict.fromkeys(source_names))\n",
    "        target_names = cluster_df['Target Entity'].to_list()\n",
    "        target_names = list(dict.fromkeys(target_names))\n",
    "        \n",
    "        source_count_dict = {}\n",
    "        target_count_dict = {}\n",
    "\n",
    "        for x in source_names:\n",
    "            count = source_count_dict.get(x, 0)\n",
    "            count += 1\n",
    "            source_count_dict[x]=count\n",
    "        \n",
    "        source_keywords = filter_words(source_names, source_count_dict, num_keywords)\n",
    "        target_keywords = filter_words(target_names, target_count_dict, num_keywords)\n",
    "        cluster_keywords.extend([{'term': x, 'type': 'source', 'cluster': i} for x in source_keywords])\n",
    "        cluster_keywords.extend([{'term': x, 'type': 'target', 'cluster': i} for x in target_keywords])\n",
    "        \n",
    "    return cluster_keywords\n",
    "\n",
    "\n",
    "def convert_and_save_keywords(df):\n",
    "    '''Save top 50 keywords per cluster to an in-memory Excel file and return it for S3 upload'''\n",
    "    keywords = get_relationship_key_words(df)  # Extract relationship keywords\n",
    "    key_df = pd.DataFrame(keywords)\n",
    "\n",
    "    # Create an in-memory Excel file\n",
    "    output = BytesIO()\n",
    "    with pd.ExcelWriter(output, engine='openpyxl') as writer:\n",
    "        key_df.to_excel(writer, sheet_name='Relationship_Keywords_By_Cluster', index=False)\n",
    "    output.seek(0)\n",
    "\n",
    "    return output\n",
    "\n",
    "# Export results to Excel and upload to S3\n",
    "def export_and_upload_results(processed_df, term_freq_df, output_file, bucket_name, s3_folder):\n",
    "    \"\"\"Save results to Excel and upload to S3.\"\"\"\n",
    "    # Save to Excel\n",
    "    output = BytesIO()\n",
    "    with pd.ExcelWriter(output, engine='openpyxl') as writer:\n",
    "        processed_df.to_excel(writer, sheet_name='Country Analysis', index=False)\n",
    "        term_freq_df.to_excel(writer, sheet_name='Term Frequencies', index=False)\n",
    "    output.seek(0)\n",
    "\n",
    "    # Upload to S3\n",
    "    s3_file_key = f\"{s3_folder}/{output_file}\"\n",
    "    s3_client.put_object(Bucket=bucket_name, Key=s3_file_key, Body=output.getvalue())\n",
    "    print(f\"üöÄ Uploaded {output_file} to s3://{bucket_name}/{s3_file_key}\")\n",
    "\n",
    "def save_combined_excel_to_s3(processed_df, term_freq_df, rel_df, keywords_df, bucket_name, s3_folder, file_name=\"combined_analysis.xlsx\"):\n",
    "    output = BytesIO()\n",
    "    with pd.ExcelWriter(output, engine='openpyxl') as writer:\n",
    "        processed_df.to_excel(writer, sheet_name=\"Country Analysis\", index=False)\n",
    "        term_freq_df.to_excel(writer, sheet_name=\"Term Frequencies\", index=False)\n",
    "        rel_df.to_excel(writer, sheet_name=\"Extracted Relationships\", index=False)\n",
    "        keywords_df.to_excel(writer, sheet_name=\"Relationship_Keywords_By_Cluster\", index=False)\n",
    "    \n",
    "    output.seek(0)\n",
    "\n",
    "    s3_file_key = f\"{s3_folder}/{file_name}\"\n",
    "\n",
    "    s3_client.put_object(Bucket=bucket_name, Key=s3_file_key, Body=output.getvalue())\n",
    "\n",
    "    print(f\"üöÄ Successfully uploaded {file_name} to s3://{bucket_name}/{s3_file_key}\")\n",
    "\n",
    "    return s3_file_key\n",
    "\n",
    "def main():\n",
    "    print(\"Starting analysis...\")\n",
    "    processed_df, term_freq_df = process_articles(news_excerpts)\n",
    "\n",
    "    # Prepare data for export\n",
    "    output_file = 'text_analysis_for_tableau.xlsx'\n",
    "    s3_folder = 'exports'\n",
    "\n",
    "    export_and_upload_results(processed_df, term_freq_df, output_file, bucket_name, s3_folder)\n",
    "\n",
    "    df = add_id_column(news_excerpts)\n",
    "\n",
    "    # Rule-based relationships extraction\n",
    "    print(\"\\nExtracting relationships...\")\n",
    "    rel_df = k_means_clustering_and_extract_relationships(df, 'Text')\n",
    "\n",
    "    keywords_df = pd.DataFrame(get_relationship_key_words(rel_df))\n",
    "\n",
    "    # Extract top 50 occurring source & target relationships\n",
    "    top_50_excel = convert_and_save_keywords(rel_df)\n",
    "\n",
    "    # Define S3 upload path for relationship keywords file\n",
    "    relationship_keywords_file = \"relationship_keywords_output.xlsx\"\n",
    "    s3_file_key = f\"{s3_folder}/{relationship_keywords_file}\"\n",
    "\n",
    "    # Upload the relationship keywords file to S3\n",
    "    s3_client.put_object(Bucket=bucket_name, Key=s3_file_key, Body=top_50_excel.getvalue())\n",
    "    print(f\"üöÄ Uploaded {relationship_keywords_file} to s3://{bucket_name}/{s3_file_key}\")\n",
    "\n",
    "\n",
    "    # Save combined Excel and upload to S3\n",
    "    combined_file_name = \"combined_analysis.xlsx\"\n",
    "    s3_path = save_combined_excel_to_s3(processed_df, term_freq_df, rel_df, keywords_df, bucket_name, s3_folder, combined_file_name)\n",
    "\n",
    "    print(f\"‚úÖ Combined Excel file uploaded to: s3://{bucket_name}/{s3_path}\")\n",
    "                                          \n",
    "    print(\"Analysis complete!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6803d65-bfe3-48ec-9d43-939a9f26011c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
